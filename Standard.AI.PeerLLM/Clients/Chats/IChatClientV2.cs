// ----------------------------------------------------------------------------------
// Copyright (c) The Standard Organization: A coalition of the Good-Hearted Engineers
// ----------------------------------------------------------------------------------

using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using Standard.AI.PeerLLM.Models.Foundations.Chats;

namespace Standard.AI.PeerLLM.Clients.Chats
{
    /// <summary>
    /// Defines the contract for interacting with the PeerLLM chat session API.
    /// Provides methods to start a chat session, stream responses, and end a chat.
    /// </summary>
    public interface IChatClientV2
    {
        /// <summary>
        /// Starts a new chat session with PeerLLM.
        /// </summary>
        /// <param name="chatSessionConfig">
        /// Configuration settings for the session, including model name, role,
        /// system prompt, target machines, fallbacks, and anti-prompts.
        /// </param>
        /// <param name="cancellationToken">
        /// A token that can be used to cancel the asynchronous operation.
        /// </param>
        /// <returns>
        /// A <see cref="Guid"/> representing the conversation ID assigned
        /// by PeerLLM for this session.
        /// </returns>
        /// <remarks>
        /// This method corresponds to the <c>POST /chats/start</c> endpoint.
        /// </remarks>
        /// <exception cref="ChatClientValidationException" />
        /// <exception cref="ChatClientDependencyException" />
        /// <exception cref="ChatClientServiceException" />
        ValueTask<Guid> StartChatAsync(
            ChatSessionConfigV2 chatSessionConfig,
            CancellationToken cancellationToken = default);

        /// <summary>
        /// Streams the chat response from PeerLLM for a given input message.
        /// </summary>
        /// <param name="conversationId">
        /// The identifier of the active conversation, obtained from
        /// <see cref="StartChatAsync"/>.
        /// </param>
        /// <param name="text">
        /// The user input or prompt text to send to the model.
        /// </param>
        /// <param name="cancellationToken">
        /// A token that can be used to cancel the streaming operation.
        /// </param>
        /// <returns>
        /// An asynchronous stream (<see cref="IAsyncEnumerable{T}"/>) of
        /// response tokens (<see cref="string"/>) generated by the model.
        /// </returns>
        /// <remarks>
        /// This method corresponds to the <c>POST /chats/stream</c> endpoint
        /// and streams output incrementally for real-time processing.
        /// </remarks>
        /// <exception cref="ChatClientValidationException" />
        /// <exception cref="ChatClientDependencyException" />
        /// <exception cref="ChatClientServiceException" />
        IAsyncEnumerable<string> StreamChatAsync(
            Guid conversationId,
            string text,
            CancellationToken cancellationToken = default);

        /// <summary>
        /// Ends an active chat session with PeerLLM.
        /// </summary>
        /// <param name="conversationId">
        /// The identifier of the conversation to end.
        /// </param>
        /// <param name="cancellationToken">
        /// A token that can be used to cancel the asynchronous operation.
        /// </param>
        /// <returns>
        /// A status message (<see cref="string"/>) returned by PeerLLM
        /// confirming that the conversation was ended successfully.
        /// </returns>
        /// <remarks>
        /// This method corresponds to the <c>POST /chats/end</c> endpoint.
        /// </remarks>
        /// <exception cref="ChatClientValidationException" />
        /// <exception cref="ChatClientDependencyException" />
        /// <exception cref="ChatClientServiceException" />
        ValueTask<string> EndChatAsync(
            Guid conversationId,
            CancellationToken cancellationToken = default);
    }
}
